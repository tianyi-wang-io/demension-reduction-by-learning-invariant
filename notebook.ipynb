{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T05:28:23.672738Z",
     "start_time": "2025-04-23T05:28:23.670483Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb166f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import get_cifar10_datasets, get_dataloader\n",
    "from train import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db9ef4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplet_images(image_label_tuple, index_to_class_mapping):\n",
    "    \"\"\"\n",
    "    Plots a row of 3 CIFAR-10 images with labels.\n",
    "\n",
    "    Args:\n",
    "        image_label_tuple (tuple): A tuple of (images, labels) where:\n",
    "            - images is a list or tensor of 3 images\n",
    "            - labels is a list or tensor of 3 corresponding labels\n",
    "        index_to_class_mapping (dict): Mapping from class index to class name\n",
    "    \"\"\"\n",
    "    images, labels = image_label_tuple\n",
    "    assert len(images) == 3\n",
    "\n",
    "    label_title = ['anchor', 'positive', 'negative']\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    for i in range(3):\n",
    "        img, label = images[i], labels\n",
    "        if i == 2:\n",
    "            display_label = 'other'\n",
    "        else:\n",
    "            display_label = index_to_class_mapping[label]\n",
    "\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().cpu()\n",
    "            if img.shape[0] == 3:  # (C, H, W)\n",
    "                img = img.permute(1, 2, 0)  # to (H, W, C)\n",
    "        \n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(label_title[i] + ': ' + display_label, fontsize=10)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5965bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_tensor, title=None):\n",
    "    assert isinstance(image_tensor, torch.Tensor), \"Input must be a torch.Tensor\"\n",
    "    assert image_tensor.shape == (3, 32, 32), \"Image must be of shape (3, 32, 32)\"\n",
    "\n",
    "    # Move to CPU, detach, and convert to numpy\n",
    "    img = image_tensor.detach().cpu()\n",
    "    img = img.permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "    # Rescale from normalized range [-1, 1] or other to [0, 1] if needed\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "    # Plot the image\n",
    "    plt.imshow(img)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914ad21",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3615f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_INDEX = [0, 1, 2, 3]\n",
    "CLASS_LABEL = ['cat', 'dog', 'ship', 'truck']\n",
    "LABEL_MAP = {original_label: new_label for new_label, original_label in enumerate(CLASS_INDEX)}\n",
    "INDEX_TO_CLASS = {i: j for i, j in zip(CLASS_INDEX, CLASS_LABEL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caeaac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering and remapping base training dataset...\n",
      "Dataset filtered. New number of samples: 20000\n",
      "Filtering and remapping base testing dataset...\n",
      "Dataset filtered. New number of samples: 4000\n",
      "Triplet Dataset created with 20000 samples (based on base dataset size).\n",
      "Classes found in dataset: [0, 1, 2, 3]\n",
      "Filtered CIFAR-10 Triplet train loader and standard test loader created with 4 workers.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = get_cifar10_datasets(data_dir='./data')\n",
    "train_loader, test_loader = get_dataloader(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd83cc4d7ef32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_triplet_images(train_dataset[random.randint(0, 20000)], INDEX_TO_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cd93ab53a5a60",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47851d16fc3f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    encoder_name='cnn',\n",
    "    embedding_dim=128,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.0005,\n",
    "    num_epochs=10,\n",
    "    triplet_margin=1,\n",
    "    triplet_weight=1,\n",
    "    classification_weight=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a85910",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    encoder_name='resnet18',\n",
    "    embedding_dim=128,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.0005,\n",
    "    num_epochs=10,\n",
    "    triplet_margin=1,\n",
    "    triplet_weight=1,\n",
    "    classification_weight=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9754410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a5041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054f670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f5fbb82",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5788eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import load_trained_model, predict_image_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890be5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cbd4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./output/cnn_4cls_128dim_model.pth\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model('./output/cnn_4cls_128dim_model.pth', 'cnn', 128, 4, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1a9b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b793e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/w4nlx9qs6d3fn7zkg65ps2bh0000gn/T/ipykernel_5392/855856324.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(model(test_dataset[1][0].to(DEVICE)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.8441e-04, 4.2710e-05, 9.9936e-01, 3.1270e-04]], device='mps:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(test_dataset[1][0].to(DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1dad719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6434,  0.9323,  0.4026,  0.0744,  0.3516,  0.4022,  0.0411,  0.2594,\n",
       "         -0.8487,  0.6498, -0.1127, -0.3098, -0.4078, -0.0682,  0.2459,  0.0418,\n",
       "         -0.2495, -0.2023,  0.1984,  0.4112, -0.2441,  0.0932,  0.0658,  0.0073,\n",
       "         -0.3816, -0.3088, -0.1498, -0.6386, -0.1389, -0.2496, -0.2576, -0.4026,\n",
       "         -0.0013, -0.4772, -0.2112, -0.2154, -0.4472, -0.5735, -0.0720, -0.2778,\n",
       "         -0.0609,  0.2521,  0.2349, -0.1334,  0.3723, -0.1490,  0.4521, -0.3317,\n",
       "         -0.6547,  0.5991,  0.2942, -0.2258, -0.7593,  0.6373,  0.2341,  0.1778,\n",
       "          0.0221,  0.0216, -0.0937, -0.2429,  0.0681, -0.3883,  0.3237,  0.4536,\n",
       "         -0.5561, -0.4577,  0.1926,  0.1559,  0.1452,  0.0410, -0.1660, -0.1513,\n",
       "          0.0107,  0.7070, -0.7750, -0.2765,  0.6864,  0.0429,  0.0619,  0.1700,\n",
       "         -0.5426,  0.1706,  0.4701, -0.5022,  0.4120, -0.2666,  0.5633,  0.1182,\n",
       "          0.1351, -0.4838, -0.1309,  0.6244, -0.6927, -0.4140,  0.2680, -0.3124,\n",
       "          0.4971, -0.3563, -0.0323, -0.2853,  0.1236, -0.0243, -0.1440, -0.2754,\n",
       "         -0.2787, -0.0882, -0.1168, -0.0627, -0.5863, -0.1372,  0.2531, -0.1145,\n",
       "         -0.6794,  0.1297, -0.5193, -0.1377, -0.0012, -0.4269, -0.2464,  0.2249,\n",
       "          0.4438,  0.1563,  0.1878,  0.1461,  0.0895, -0.3786,  0.2125,  0.8144]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_embedding(test_dataset[1][0].to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99abcf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(test_dataset[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d2cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37aa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca1a077deda1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956d23aa445b1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af53060e2ea4078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac613e233910fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a1dc05a9d6ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
